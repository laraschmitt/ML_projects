{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "                            recall_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "# Set figure size to (12, 6)\n",
    "plt.rcParams['figure.figsize'] = (12,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class imbalance\n",
    "\n",
    "- What is it?\n",
    "- Why might we care about it?\n",
    "- How can we deal with it?\n",
    "\n",
    "\n",
    "## What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a creditcard dataset\n",
    "df = pd.read_csv('creditcardfraud.zip', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the class sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentage of observations belonging to class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it a Problem\n",
    "\n",
    "- Because the classifier has an \"incentive\" to rather predict class 0. It just shows up so much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we deal with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that prints scores and a confusion matrix for a specified model\n",
    "def print_evaluations(ytrue, ypred, model):\n",
    "    '''\n",
    "    Prints the confusion matrix and some evaluation metrics for \n",
    "    a specified model.\n",
    "    '''\n",
    "    print(f'How does model {model} score:')\n",
    "    print(f'The accuracy of the model is: {round(accuracy_score(ytrue, ypred), 3)}')\n",
    "    print(f'The precision of the model is: {round(precision_score(ytrue, ypred), 3)}')\n",
    "    print(f'The recall of the model is: {round(recall_score(ytrue, ypred), 3)}')\n",
    "    print(f'The f1-score of the model is: {round(f1_score(ytrue, ypred), 3)}')\n",
    "    \n",
    "    #print confusion matrix\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    cm = confusion_matrix(ytrue, ypred)\n",
    "    print(cm)\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='.0f', ax= ax)\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(['non-fraud', 'fraud'])\n",
    "    ax.yaxis.set_ticklabels(['non-fraud', 'fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_bl = [0] * X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluations(y, ypred_bl, 'Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare it to a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, max_depth=3\n",
    "                            , random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "ypred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the evaluators\n",
    "print_evaluations(y_test, ypred_rf, 'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate both resamplers\n",
    "rus = ...\n",
    "nm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample according to both resamplers\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "X_nm, y_nm = nm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rus.shape, y_rus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random undersampling model, \n",
    "# make predictions and inspect evaluations\n",
    "rf.fit(X_rus, y_rus)\n",
    "ypred_rus = rf.predict(X_test)\n",
    "print_evaluations(y_test, ypred_rus, 'RandomUndersampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Near Miss, \n",
    "# make predictions and inspect evaluations\n",
    "rf.fit(X_nm, y_nm)\n",
    "ypred_nm = rf.predict(X_test)\n",
    "print_evaluations(y_test, ypred_nm, 'NearMiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomOverSampler Model\n",
    "ros = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_ros, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the RandomOverSampling, \n",
    "# make predictions and inspect evaluations\n",
    "rf.fit(X_ros, y_ros)\n",
    "ypred_ros = rf.predict(X_test)\n",
    "print_evaluations(y_test, ypred_ros, 'RandomOversampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and run SMOTE\n",
    "sm = SMOTE(sampling_strategy={1: 2000})\n",
    "X_sm, y_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "rf.fit(X_sm, y_sm)\n",
    "ypred_sm = rf.predict(X_test)\n",
    "print_evaluations(y_test, ypred_sm, 'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
