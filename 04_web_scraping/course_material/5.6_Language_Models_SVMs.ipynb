{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Background and Theory:\n",
    "#### * Categories and applications of NLP\n",
    "#### * What is a Language model?\n",
    "#### * See a Rule based and a Statistically based model\n",
    "#### * Problems in Language Representation\n",
    "\n",
    "### Applications for this week\n",
    "#### * Spacy Basic - solving Preprocessing\n",
    "#### * Spacy Advanced - Introducing word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories and applications of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Language Model?\n",
    "#### Either rule based or statistically based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based:\n",
    "#### Codify your expertise on what constitutes 'correct' and 'incorrect' langauge\n",
    "* Good for specific & limited examples\n",
    "* Lots of customisation\n",
    "* Linguists love it!\n",
    "* e.g. VADER, Spacy\n",
    "\n",
    "### Rule-based example:\n",
    "* A sentiment analysis function using simple sentiment counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape positive words\n",
    "positive = requests.get('https://www.thesaurus.com/browse/good')\n",
    "text = soup(positive.text, parser='html.parser')\n",
    "synonyms = text.find(attrs={\"class\":\"css-1ytlws2 et6tpn80\"})\n",
    "pos_words = [a.text for a in synonyms.find_all('a')]\n",
    "keywords['positive'] = pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape negative words\n",
    "negative = requests.get('https://www.thesaurus.com/browse/bad')\n",
    "text = soup(negative.text, parser='html.parser')\n",
    "synonyms = text.find(attrs={\"class\":\"css-1ytlws2 et6tpn80\"})\n",
    "neg_words = [a.text for a in synonyms.find_all('a')]\n",
    "keywords['negative'] = neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acceptable',\n",
       " 'bad',\n",
       " 'excellent',\n",
       " 'exceptional',\n",
       " 'favorable',\n",
       " 'great',\n",
       " 'marvelous',\n",
       " 'positive',\n",
       " 'satisfactory',\n",
       " 'satisfying',\n",
       " 'superb',\n",
       " 'valuable',\n",
       " 'wonderful',\n",
       " 'ace',\n",
       " 'boss',\n",
       " 'bully',\n",
       " 'capital',\n",
       " 'choice',\n",
       " 'crack',\n",
       " 'nice',\n",
       " 'pleasing',\n",
       " 'prime',\n",
       " 'rad',\n",
       " 'sound',\n",
       " 'spanking',\n",
       " 'sterling',\n",
       " 'super',\n",
       " 'superior',\n",
       " 'welcome',\n",
       " 'worthy',\n",
       " 'admirable',\n",
       " 'agreeable',\n",
       " 'commendable',\n",
       " 'congenial',\n",
       " 'deluxe',\n",
       " 'first-class',\n",
       " 'first-rate',\n",
       " 'gnarly',\n",
       " 'gratifying',\n",
       " 'honorable',\n",
       " 'neat',\n",
       " 'precious',\n",
       " 'recherché',\n",
       " 'reputable',\n",
       " 'select',\n",
       " 'shipshape',\n",
       " 'splendid',\n",
       " 'stupendous',\n",
       " 'super-eminent',\n",
       " 'super-excellent',\n",
       " 'tip-top',\n",
       " 'up to snuff']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_or_negative(text, keywords):\n",
    "        \"\"\"calculate the sentiment of a string based on word count of positive and negative terms\n",
    "\n",
    "        params: text - the string to be assessed \n",
    "                keywords - a dictionary of keywords\n",
    "\n",
    "        returns: classification either positive or negative\"\"\"\n",
    "        \n",
    "        try:\n",
    "            text = re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b',text.lower())\n",
    "            positives = [pos for pos in text if pos in keywords['positive']]\n",
    "            negatives = [neg for neg in text if neg in keywords['negative']]\n",
    "            return 'positive' if len(positives) >= len(negatives) else 'negative' \n",
    "        except:\n",
    "            return 'Sorry something went wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit is not a good idea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_or_negative(input(), keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit is an atrocious idea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_or_negative(input(), keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"background-color:orange\">**-> Rule based systems are easy to build, but hard to get good results from**</span>\n",
    "- double meanings break systems\n",
    "- humour and sarcasm breaks system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Semantics - I go home vs I go house - MEANING\n",
    "* Syntax - I go home vs I goes home - GRAMMER\n",
    "* Pragmatics - I go home vs I go home cry myself to sleep because I am depressed - CONTENT - Grices maxims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistically-based: \n",
    "#### Build a model which tries to assess whether language is 'likely' or 'unlikely' regarding semantics, syntax and pragmatics\n",
    "\n",
    "* Good for less well-defined uses\n",
    "* Harder to customise \n",
    "* Computer Scientists love it!\n",
    "* e.g. Spacy, HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(W) = P(w_1, w_2, ..., w_n)$ # seuqence classification \n",
    "\n",
    "or\n",
    "\n",
    "$P(w_{t+1} | w_{t-1+n}, ..., w_{t})$ # Sequence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistically-based example:\n",
    "* A bigram Markov model which generates new language (Sequence Generation)\n",
    "* Downloads text data and calculates probabilities based on that\n",
    "* Use the probability distributions to calculate new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_distribution(data):\n",
    "    \"\"\"create a probability distribution over all words\n",
    "    \n",
    "        params: data - a Bunch data object from sklearn\n",
    "        returns: Word probability distribution\n",
    "    \"\"\"\n",
    "\n",
    "    text = data['data']\n",
    "    all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w+\\\\b',article.lower())) for article in text]).split()\n",
    "    words = pd.DataFrame({'words':all_data})\n",
    "    words['next_words'] = words['words'].shift(-1)\n",
    "    word_distribution = words.groupby('words')['next_words'].value_counts(normalize=True)\n",
    "    \n",
    "    return word_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation(seed, length, distribution):\n",
    "    \"\"\"seed a distribution with a seed word, and ask it to make more words\n",
    "        \n",
    "        params: seed - A seed word, \n",
    "                length -Length of the generated sentence\n",
    "                distribution - A word probability distribution\n",
    "                \n",
    "        returns: generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        seed = seed.lower()\n",
    "        for i in range(length):\n",
    "             seed += ' ' + np.random.choice(distribution[seed.split()[-1]].index, p=distribution[seed.split()[-1]].values)\n",
    "        return seed\n",
    "    \n",
    "    except:\n",
    "        print('Oops! Try another seed')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['header', 'footer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the bigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = generate_word_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words     next_words\n",
       "00        00            0.112125\n",
       "          gmt           0.035854\n",
       "          1993          0.021512\n",
       "          01            0.018905\n",
       "          am            0.014342\n",
       "                          ...   \n",
       "érale     et            1.000000\n",
       "ête       renvers       1.000000\n",
       "íålittin  no            1.000000\n",
       "ñaustin   jacobs        1.000000\n",
       "ýé        am            1.000000\n",
       "Name: next_words, Length: 1054743, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while driving course in the penguins the israeli patrols every time year although it was about flopticals could use 32 bits long exposure most of speech research council of china or cipriani att com coffee as saying shouldn think that make size up in assuming of the university of comfort and'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation('While', 50, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Staitiscal based models:\n",
    "- less precise - harder to control\n",
    "- they learn themselves\n",
    "- require good initialization\n",
    "- require lots of data to work well\n",
    "- THE BEST STATISTICAL MODELS ARE DEEP MODELS - NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems in Language Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Preprocessing - tokenization, stop words, lowercase, lemmatization/stemming\n",
    "#### * Curse of dimensionality\n",
    "#### * Semantic similarity\n",
    "#### * Word order\n",
    "#### * Word sense disambiguation\n",
    "#### * Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'was', 'an', 'untokenized', 'string']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I was an untokenized string'.split() # tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and if but on a'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'and if but on a' # stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Apple is a fruit', 'My fruit is an apple')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Apple is a fruit', 'My fruit is an apple' # always lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'To be or not to be that is the question whether tis nobler to suffer the sling and arrows'\n",
    "'is' > 'be' 'am' > 'be'# lemmatization\n",
    "'slings' > 'sling' # stemming - find the root (lexeme) of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications for this week:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Basic - solving Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `pip install spacy`\n",
    "* `python -m spacy download en_core_web_md` - if this doesn't load, try\n",
    "* `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I can transform this string"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = nlp('I can transform these strings')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NOUN', False, 'string')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-1].pos_, result[-1].is_stop, result[-1].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(corpus, model):\n",
    "    \"\"\"preprocess a string (tokens, stopwords, lowercase, lemma & stemming) returns the cleaned result\n",
    "        params: review - a string\n",
    "                model - a spacy model\n",
    "                \n",
    "        returns: list of cleaned strings\n",
    "    \"\"\"\n",
    "    \n",
    "    new_doc = []\n",
    "    doc = model(corpus)\n",
    "    for word in doc:\n",
    "        if not word.is_stop and word.is_alpha:\n",
    "            new_doc.append(word.lemma_.lower())\n",
    "            \n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transform', 'string']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('I can transform these strings', nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity - None in BOW!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Advanced - introducing word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solves: Semantic similarity, curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_my_word(word, model):\n",
    "    try:\n",
    "        return model.vocab[word].vector.reshape(-1,1).T\n",
    "    except:\n",
    "        print(\"Doesn't look like this word can be found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
